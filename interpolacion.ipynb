import pandas as pd
import numpy as np
from scipy.interpolate import griddata

def load_and_validate_data(file_paths):
    dataframes = {}
    required_columns = ['id_punto', 'longitud', 'latitud']
    for source, path in file_paths.items():
        try:
            df = pd.read_csv(path)
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"Faltan columnas requeridas en {source}")
            dataframes[source] = df
            print(f"Datos de {source} cargados exitosamente: {df.shape[0]} registros")
        except Exception as e:
            print(f"Error al cargar {source}: {str(e)}")
    return dataframes

def interpolate_spatial_resolution(df, target_res=500):
    # Crear una malla regular para la interpolación
    x = np.arange(df['longitud'].min(), df['longitud'].max(), target_res / 111111.0)
    y = np.arange(df['latitud'].min(), df['latitud'].max(), target_res / 111111.0)
    xi, yi = np.meshgrid(x, y)
    
    # Identificar columnas numéricas para interpolar
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col not in ['longitud', 'latitud', 'id_punto']]
    
    # Realizar interpolación para cada variable
    interpolated_data = []
    for col in numeric_cols:
        zi = griddata((df['longitud'], df['latitud']), df[col], (xi, yi), method='linear')
        interpolated_data.append(zi)
    
    # Crear nuevo DataFrame con los datos interpolados
    new_points = []
    for i in range(yi.shape[0]):  # Corregido: usar yi.shape[0]
        for j in range(xi.shape[1]):  # Corregido: usar xi.shape[1]
            point = {'longitud': xi[i,j], 'latitud': yi[i,j]}
            for k, col in enumerate(numeric_cols):
                if interpolated_data[k] is not None:  # Verificar que hay datos interpolados
                    point[col] = interpolated_data[k][i,j]
                else:
                    point[col] = np.nan
            new_points.append(point)
    
    return pd.DataFrame(new_points)

def integrate_datasets(file_paths, target_resolution=500, temporal_freq='M'):
    # Cargar y validar los datos
    datasets = load_and_validate_data(file_paths)
    
    # Realizar interpolación espacial para cada conjunto de datos
    interpolated_datasets = {}
    for source, df in datasets.items():
        try:
            interpolated_datasets[source] = interpolate_spatial_resolution(df, target_resolution)
            print(f"Interpolación espacial completada para {source}")
        except Exception as e:
            print(f"Error en la interpolación de {source}: {str(e)}")
            continue
    
    # Combinar los conjuntos de datos interpolados
    merged_df = None
    for source, df in interpolated_datasets.items():
        if merged_df is None:
            merged_df = df
        else:
            merged_df = merged_df.merge(df, on=['longitud', 'latitud'], how='outer', 
                                      suffixes=(f'_{source}', ''))
    
    return merged_df

# Uso del script
if __name__ == "__main__":
    # Configurar las rutas de archivos
    file_paths = {
        'era5': '/content/drive/MyDrive/data/era5_mexico_data.csv',
        'modis': '/content/drive/MyDrive/data/modis_mexico_data.csv',
        'srtm': '/content/drive/MyDrive/data/srtm_mexico_data.csv',
        'chirps': '/content/drive/MyDrive/data/variables_mexico_completo.csv'
    }

    # Integrar y guardar el resultado
    resultado = integrate_datasets(file_paths, target_resolution=500, temporal_freq='M')
    if resultado is not None:
        resultado.to_csv('/content/drive/MyDrive/data/datos_consolidados_mexico.csv', index=False)
        print("Integración completada y archivo guardado como 'datos_consolidados_mexico.csv'")
    else:
        print("Error: No se pudo completar la integración de datos")
